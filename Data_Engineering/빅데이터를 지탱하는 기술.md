# 빅데이터를 지탱하는 기술

## 빅데이터 기초 지식

빅데이터의 취급이 어려운 이유는 크게 두가지다. 하나는 '데이터의 분석 방법을 모른다'는 점이고, 또 하나의 이유는 '데이터 처리에 수고와 시간이 걸린다'는 점이다.
_Hadoop_ 은 '다수의 컴퓨터에서 대량의 데이터를 처리하기'위한 시스템이다.

가속도적으로 늘어나는 데이터의 처리는 Hadoop에 맡기고 비교적 작은 데이터, 또는 중요한 데이터만을 데이터 웨어하우스에 넣는 식으로 사용을 구분하게 되었다.
Amazon Redshift 이후로 데이터 웨어하우스를 클라우드 상에서 작성하는 것은 그다지 드문일이 아니게 되었다.

스몰데이터(한 대의 노트북에서 큰 부담없이 처리할 수 있을 만큼의 작은 데이터, 수 GB) 기술은 데이터양이 증가하면 처리 시간이 급격히 증가한다.
빅데이터의 경우 데이터의 양이 적은 상황에서는 스몰 데이터 기술이 더 우수하다.

## 2020.12.05

일반적으로 차례대로 전달해나가는 데이터로 구성된 시스템을 '__데이터 파이프라인__'이라고 한다.

빅데이터 기술이 기존의 데이터 웨어하우스와 다른 점은 다수의 분산 시스템을 조합하여 확장성이 뛰어난 데이터 처리 구조를 만든다는 점이다.

대량의 데이터를 수집한 후 분산처리하는 것이 좋다. 

분산 스토리지는 여러 컴퓨터와 디스크로부터 구성된 스토리지 시스템을 말한다.

그 대표적인 것이 객체 스토리지이고, 유명한 서비스로는 Amazon S3가 있다.

데이터를 저장하였으면 분산 스토리지에서 추출한 데이터를 데이터 웨어하우스에 적합한 형식으로 변환한다. -> 이 작업이 ETL 프로세스

데이터 웨어하우스는 대량의 데이터를 장기보존하는데에 초점이 맞추어져 있다. (Amazon Redshift)  
(하루에 몇번씩 소량으로 꺼내어 자주 쓰기 보다는 하루가 끝날 때 정리하는 정도)

데이터 분석을 위한 데이터들은 (자주 사용하고, 과부화 걸리지 않게 하기 위해) 데이터 마트에 저장한다.

많은 데이터를 우선 저장하고 보는게 데이터 레이크이다. -> 단순 스토리지 (아직 데이터 웨어하우스에 넣을 수 없는 데이터)

항상 데이터가 먼저 있고, 테이블을 나중에 설계하는 것이 빅데이터

__데이터 파이프라인의 큰 흐름은 변하지 않는다.__  
+ 저장할 수 있는 데이터의 제한이 없을 것
+ 데이터를 효율적으로 추출할 수단이 있을 것

데이터 파이프라인 흐름: 데이터 수집 -> 분산 스토리지 -> 분산 데이터 처리 -> 데이터 마트 -> 시각화 도구

데이터 수집의 목적으로는 크게 3가지가 있다.

1. 데이터 검색: 만약 시스템에 장애가 발생하였을 때 원인을 찾거나 고객으로부터 문의가 있을 경우 로그 탐색하는 경우 등. 어떤것이 필요할지 모르기 때문에 모든 데이터를 취득

2. 데이터 가공: 추천 상품을 제안하거나 센서 데이터에서 비정상적인 상태를 감지해서 통보하는 경우 등등. 필요한 데이터를 사전에 모아 설계해둠. -> 데이터 가공에는 자동화가 필수

3. 데이터 시각화: 데이터를 시각적으로 봄으로써 얻고 싶은 정보가 있는 경우. 통계 SW나 BI 툴 활용해 의사결정에 도움. 
